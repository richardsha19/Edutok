{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888fa411-2029-405c-9ac4-6eed87ae3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete...\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !echo \"Installation in progress...\"\n",
    "# !conda install -y --quiet  --prefix {sys.prefix} -c conda-forge \\\n",
    "#     #accelerate==0.23.0 \\\n",
    "#     #validators==0.22.0 \\\n",
    "#     #diffusers==0.18.2 \\\n",
    "#     #transformers==4.32.1 \\\n",
    "#     pillow \\\n",
    "#     PyPDF2 \\\n",
    "#     numpy \\\n",
    "#     openai \\\n",
    "#     ipython > /dev/null && echo \"Installation successful\" || echo \"Installation failed\"\n",
    "\n",
    "!{sys.executable} -m !pip install invisible-watermark --user > /dev/null 2>&1\n",
    "!{sys.executable} -m !pip install transformers huggingface-hub --user > /dev/null 2>&1\n",
    "!{sys.executable} -m !pip install PyPDF2 --user > /dev/null 2>&1\n",
    "!{sys.executable} -m !pip install numpy --user > /dev/null 2>&1\n",
    "#!{sys.executable} -m !pip install openai==1.12.0 --user > /dev/null 2>&1\n",
    "!{sys.executable} -m !pip install typing-extensions==3.10.0.2 --user > /dev/null 2>&1\n",
    "!echo \"Installation complete...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bff3304-4b47-48ee-9c61-dc2f9e949a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing-extensions==4.7\n",
      "  Using cached typing_extensions-4.7.0-py3-none-any.whl (33 kB)\n",
      "Collecting openai==1.12\n",
      "  Using cached openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai==1.12)\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.12)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.12)\n",
      "  Using cached httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "Collecting sniffio (from openai==1.12)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai==1.12)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai==1.12)\n",
      "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai==1.12)\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai==1.12)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.12)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.12)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.16.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "INFO: pip is looking at multiple versions of pydantic-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.14.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.14.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.5.1-py3-none-any.whl (381 kB)\n",
      "Collecting pydantic-core==2.14.3 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.14.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "Collecting pydantic-core==2.14.1 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.4.1-py3-none-any.whl (395 kB)\n",
      "INFO: pip is looking at multiple versions of pydantic-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pydantic-2.4.0-py3-none-any.whl (395 kB)\n",
      "Collecting pydantic-core==2.10.0 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "Collecting pydantic-core==2.6.3 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.2.1-py3-none-any.whl (373 kB)\n",
      "Collecting pydantic-core==2.6.1 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.2.0-py3-none-any.whl (373 kB)\n",
      "Collecting pydantic-core==2.6.0 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "Collecting pydantic-core==2.4.0 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.1.0-py3-none-any.whl (370 kB)\n",
      "  Using cached pydantic-2.0.3-py3-none-any.whl (364 kB)\n",
      "Collecting pydantic-core==2.3.0 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.0.2-py3-none-any.whl (359 kB)\n",
      "Collecting pydantic-core==2.1.2 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.0.1-py3-none-any.whl (358 kB)\n",
      "Collecting pydantic-core==2.0.2 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.12)\n",
      "  Using cached pydantic-2.0-py3-none-any.whl (355 kB)\n",
      "Collecting pydantic-core==2.0.1 (from pydantic<3,>=1.9.0->openai==1.12)\n",
      "  Using cached pydantic_core-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Installing collected packages: typing-extensions, tqdm, sniffio, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.0\n",
      "    Uninstalling typing_extensions-4.7.0:\n",
      "      Successfully uninstalled typing_extensions-4.7.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/u9f6f4ec885f75323c3ca10763484500/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.0\n",
      "    Uninstalling sniffio-1.3.0:\n",
      "      Successfully uninstalled sniffio-1.3.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.0\n",
      "    Uninstalling exceptiongroup-1.2.0:\n",
      "      Successfully uninstalled exceptiongroup-1.2.0\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.9.0\n",
      "    Uninstalling distro-1.9.0:\n",
      "      Successfully uninstalled distro-1.9.0\n",
      "\u001b[33m  WARNING: The script distro is installed in '/home/u9f6f4ec885f75323c3ca10763484500/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.6.0\n",
      "    Uninstalling annotated-types-0.6.0:\n",
      "      Successfully uninstalled annotated-types-0.6.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.0.1\n",
      "    Uninstalling pydantic_core-2.0.1:\n",
      "      Successfully uninstalled pydantic_core-2.0.1\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.0\n",
      "    Uninstalling pydantic-2.0:\n",
      "      Successfully uninstalled pydantic-2.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "\u001b[33m  WARNING: The script httpx is installed in '/home/u9f6f4ec885f75323c3ca10763484500/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.12.0\n",
      "    Uninstalling openai-1.12.0:\n",
      "      Successfully uninstalled openai-1.12.0\n",
      "\u001b[33m  WARNING: The script openai is installed in '/home/u9f6f4ec885f75323c3ca10763484500/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-4.2.0 certifi-2024.2.2 distro-1.9.0 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 openai-1.12.0 pydantic-2.0 pydantic-core-2.0.1 sniffio-1.3.0 tqdm-4.66.2 typing-extensions-4.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall typing-extensions==4.7 openai==1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d5daf7-f69e-4227-a8d1-f9769878345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe34e04-f105-444a-beee-c25c70742e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "from io import BytesIO\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Suppress warnings for a cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import intel_extension_for_pytorch as ipex  # Used for optimizing PyTorch models\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "#import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7c3efd-ac4a-456d-a9d1-917686813c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THe class\n",
    "\n",
    "class Text2ImgModel:\n",
    "    \"\"\"\n",
    "    Text2ImgModel is a class for generating images based on text prompts using a pretrained model.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default to \"xpu\" - Intel dGPUs.\n",
    "    - pipeline: The loaded model pipeline.\n",
    "    - data_type: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_id_or_path: str,\n",
    "            device: str = \"cpu\",\n",
    "            torch_dtype: torch.dtype = torch.bfloat16,\n",
    "            optimize: bool = True,\n",
    "            enable_scheduler: bool = False,\n",
    "            warmup: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for Text2ImgModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - device: The device to run the model on. Default is \"xpu\".\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: Whether to optimize the model after loading. Default is True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.pipeline = self._load_pipeline(\n",
    "            model_id_or_path, torch_dtype, enable_scheduler\n",
    "        )\n",
    "        self.data_type = torch_dtype\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            # print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            # print(\n",
    "            #    \"Optimization completed in {:.2f} seconds.\".format(\n",
    "            #        time.time() - start_time\n",
    "            #    )\n",
    "            # )\n",
    "        if warmup:\n",
    "            self.warmup_model()\n",
    "\n",
    "    def _load_pipeline(\n",
    "            self,\n",
    "            model_id_or_path: str,\n",
    "            torch_dtype: torch.dtype,\n",
    "            enable_scheduler: bool,\n",
    "\n",
    "    ) -> DiffusionPipeline:\n",
    "        \"\"\"\n",
    "        Loads the pretrained model and prepares it for inference.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The loaded model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Loading the model...\")\n",
    "        model_path = Path(f\"Big_Data/GenAI/{model_id_or_path}\")\n",
    "\n",
    "        if model_path.exists():\n",
    "            # print(f\"Loading the model from {model_path}...\")\n",
    "            load_path = model_path\n",
    "        else:\n",
    "            print(\"Using the default path for models...\")\n",
    "            load_path = model_id_or_path\n",
    "\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            load_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "        if enable_scheduler:\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "        if not model_path.exists():\n",
    "            try:\n",
    "                print(f\"Attempting to save the model to {model_path}...\")\n",
    "                pipeline.save_pretrained(f\"{model_path}\")\n",
    "                print(\"Model saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving the model: {e}. Proceeding without saving.\")\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        # print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def _optimize_pipeline(self, pipeline: DiffusionPipeline) -> DiffusionPipeline:\n",
    "        \"\"\"\n",
    "        Optimizes the model for inference using ipex.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline: The model pipeline to be optimized.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The optimized model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        return pipeline\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample image.\n",
    "        \"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        start_time = time.time()\n",
    "        self.generate_images(\n",
    "            prompt=\"A beautiful sunset over the mountains\",\n",
    "            num_images=1,\n",
    "            save_path=\".tmp\",\n",
    "        )\n",
    "        print(\n",
    "            \"Model is set up and ready! Warm-up completed in {:.2f} seconds.\".format(\n",
    "                time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimizes the current model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def generate_images(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            num_inference_steps: int = 50,\n",
    "            num_images: int = 5,\n",
    "            save_path: str = \"output\",\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images based on the given prompt and saves them to disk.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The text prompt to generate images from.\n",
    "        - num_inference_steps: Number of noise removal steps.\n",
    "        - num_images: The number of images to generate. Default is 5.\n",
    "        - save_path: The directory to save the generated images in. Default is \"output\".\n",
    "\n",
    "        Returns:\n",
    "        - images: A list of the generated images.\n",
    "        \"\"\"\n",
    "\n",
    "        images = []\n",
    "        for i in range(num_images):\n",
    "            with torch.cpu.amp.autocast(\n",
    "                    enabled=True if self.data_type != torch.float32 else False,\n",
    "                    dtype=self.data_type,\n",
    "            ):\n",
    "                image = self.pipeline(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    # negative_prompt=negative_prompt,\n",
    "                ).images[0]\n",
    "                if not os.path.exists(save_path):\n",
    "                    try:\n",
    "                        os.makedirs(save_path)\n",
    "                    except OSError as e:\n",
    "                        print(\"Failed to create directory\", save_path, \"due to\", str(e))\n",
    "                        raise\n",
    "            output_image_path = os.path.join(\n",
    "                save_path,\n",
    "                f\"{'_'.join(prompt.split()[:3])}_{i}_{sum(ord(c) for c in prompt) % 10000}.png\",\n",
    "            )\n",
    "            image.save(output_image_path)\n",
    "            images.append(image)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c0e91e-c8a5-4b42-ac54-e94becb90019",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prompt_to_image\n",
    "\n",
    "model_cache = {}\n",
    "\n",
    "def prompt_to_image(prompt_arr):\n",
    "    output_dir = \"output\"\n",
    "    model_ids = [\n",
    "        \"stabilityai/stable-diffusion-2-1\",\n",
    "        \"CompVis/stable-diffusion-v1-4\",\n",
    "    ]\n",
    "\n",
    "    enhance_checkbox_value = False\n",
    "    num_images = 1\n",
    "\n",
    "    enhancements = [\n",
    "        \"historical\"\n",
    "    ]\n",
    "\n",
    "    if not prompt_arr:\n",
    "        prompt_arr = []\n",
    "\n",
    "    for model_id in model_ids:\n",
    "        model_key = (model_id, \"cpu\") # Changed from xpu\n",
    "        if model_key not in model_cache:\n",
    "            model_cache[model_key] = Text2ImgModel(model_id, device=\"cpu\") # Changed from xpu\n",
    "\n",
    "        model = model_cache[model_key]\n",
    "\n",
    "        try:\n",
    "            for prompt_text in prompt_arr:\n",
    "                if enhance_checkbox_value:\n",
    "                    prompt_text = prompt_text + \" \" + \" \".join(random.sample(enhancements, 5))\n",
    "                    print(f\"Using enhanced prompt: {prompt_text}\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                model.generate_images(\n",
    "                    prompt_text,\n",
    "                    num_images=num_images,\n",
    "                    save_path=\"./output\",\n",
    "                )\n",
    "                print(f\"Complete generating {num_images} images in './output' in {time.time() - start_time:.2f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa974133-a556-47ef-affb-562a7d6f1fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 21:45:25,324 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Textbook Content: behaviorism\n",
      "the view that psychology (1) should be an objective science that (2) studies\n",
      "behavior without reference to mental processes. Most psychologists today agree\n",
      "with (1) but not with (2).\n",
      "Freudian (Psychoanalytic) Psychology\n",
      "The other major force was \n",
      "Sigmund Freud\n",
      "’s psychoanalytic psychology,\n",
      "which emphasized the ways our unconscious mind and childhood\n",
      "experiences affect our behavior. (In later modules, we’ll look more closely\n",
      "at Freud’s teachings, including his theory of personality and his views on\n",
      "unconscious sexual conflicts and the mind’s defenses against its own\n",
      "wishes and impulses.)\n",
      "John B. Watson (1878–1958) and Rosalie Rayner (1898–1935)\n",
      "Working with Rayner, Watson championed psychology as the scientific study of\n",
      "behavior. In a controversial study on a baby who became famous as “Little\n",
      "Albert,” he and Rayner showed that fear could be learned. (More about this in\n",
      "Module 26\n",
      ".)\n",
      "87B. F. Skinner (1904–1990)\n",
      " \n",
      "This leading behaviorist rejected introspection\n",
      "and studied how consequences shape behavior.\n",
      "Sigmund Freud (1856–1939)\n",
      " \n",
      "The controversial ideas of this famed\n",
      "personality theorist and therapist have influenced humanity’s self-understanding.\n",
      "Humanistic Psychology\n",
      "As the behaviorists had rejected the early 1900’s definition of \n",
      "psychology,\n",
      "other groups rejected the behaviorist definition. In the 1960s, \n",
      "humanistic\n",
      "psychologists\n",
      ",\n",
      " led by \n",
      "Carl Rogers\n",
      " and \n",
      "Abraham Maslow\n",
      ", found both\n",
      "behaviorism and Freudian psychology too limiting. Rather than focusing\n",
      "on conditioned responses or childhood memories, the humanistic\n",
      "psychologists focused on our potential for personal growth. (More about\n",
      "the humanistic psychologists in \n",
      "Module 57\n",
      ".)\n",
      "humanistic psychology\n",
      "a historically significant perspective that emphasized human growth potential.\n",
      "88\n",
      "========================================================\n",
      "\n",
      "Summarized Content: John B. Watson, Rosalie Rayner, Sigmund Freud, and B. F. Skinner were major influences on the development of psychology. \n",
      "========================================================\n",
      "\n",
      "ChatGPT's catchy response: Uncover the captivating minds behind the evolution of psychology! From the controversial experiments of John B. Watson and Rosalie Rayner to the revolutionary theories of Sigmund Freud and B. F. Skinner, explore the fascinating journey of this influential field.\n",
      "['Uncover the captivating minds behind the evolution of psychology! From the controversial experiments of John B', ' Watson and Rosalie Rayner to the revolutionary theories of Sigmund Freud and B', ' F', ' Skinner, explore the fascinating journey of this influential field']\n",
      "Loading the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a5a915915842398255f9b46283a649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_string(input):\n",
    "    '''\n",
    "    Function: Parsing the json object we receive from firebase.\n",
    "    Structure of json file: an array of key-value pairs, with keys being page numbers\n",
    "    :return: array of strings\n",
    "    '''\n",
    "\n",
    "    # Test\n",
    "    string_txt = \"Barack Obama giving his inauguration speech.* The fall of the Berlin Wall.* A depiction of the Industrial Revolution, focusing on its impact on rural communities.* The Battle of Gettysburg from the viewpoint of a Confederate soldier.*\"\n",
    "\n",
    "    # parse json\n",
    "    special_char = '.'\n",
    "\n",
    "    str_arr = input.split(special_char)\n",
    "    return str_arr[:-1]\n",
    "\n",
    "\n",
    "def pdf_gen():\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader('ap-psych.pdf')\n",
    "\n",
    "    startingPageNo = 87  # TODO: input from the frontend\n",
    "    endingPageNo = 89  # TODO: input from the frontend\n",
    "\n",
    "    textOutput = \"\"\n",
    "\n",
    "    for currPageNo in range(startingPageNo + 1 - 2, endingPageNo + 1 - 2):\n",
    "        # getting a specific page from the pdf file\n",
    "        page = reader.pages[currPageNo]\n",
    "\n",
    "        # extracting text from page\n",
    "        textOutput += page.extract_text()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lidiya/bart-large-xsum-samsum\", use_fast=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-large-xsum-samsum\")\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Make the summarized content catchy\n",
    "\n",
    "    openai.api_key = \"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\"\n",
    "\n",
    "    summarizedTextOutput = summarizer(textOutput)\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"don't use emojis but make the content sound intriguing with an attention catching opening in 33 words. add a relevant factual information that might help users understand the concept better.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": summarizedTextOutput[0][\"summary_text\"]\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=64,\n",
    "        top_p=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nOriginal Textbook Content:\", textOutput)\n",
    "    print(\"========================================================\")\n",
    "    print(\"\\nSummarized Content:\", summarizedTextOutput[0][\"summary_text\"])\n",
    "    print(\"========================================================\")\n",
    "    print(\"\\nChatGPT's catchy response:\", response.choices[0].message.content)\n",
    "\n",
    "    str_arr = split_string(response.choices[0].message.content)\n",
    "\n",
    "    print(str_arr)\n",
    "\n",
    "    # Call the prompt_to_image()\n",
    "    prompt_to_image(str_arr)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pdf_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df08ddd-fc07-4498-b86d-715a91bdf7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
